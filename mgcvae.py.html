<html>
<head>
<title>mgcvae.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cc7832;}
.s1 { color: #a9b7c6;}
.s2 { color: #6897bb;}
.s3 { color: #6a8759;}
.s4 { color: #808080;}
.s5 { color: #629755; font-style: italic;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
mgcvae.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">warnings</span>
<span class="s0">import </span><span class="s1">torch.nn </span><span class="s0">as </span><span class="s1">nn</span>
<span class="s0">import </span><span class="s1">torch.nn.functional </span><span class="s0">as </span><span class="s1">F</span>
<span class="s0">import </span><span class="s1">torch.optim </span><span class="s0">as </span><span class="s1">optim</span>
<span class="s0">from </span><span class="s1">trajectron.model.components </span><span class="s0">import </span><span class="s1">*</span>
<span class="s0">from </span><span class="s1">trajectron.model.model_utils </span><span class="s0">import </span><span class="s1">*</span>
<span class="s0">import </span><span class="s1">trajectron.model.dynamics </span><span class="s0">as </span><span class="s1">dynamic_module</span>
<span class="s0">from </span><span class="s1">trajectron.environment.scene_graph </span><span class="s0">import </span><span class="s1">DirectedEdge</span>


<span class="s0">class </span><span class="s1">MultimodalGenerativeCVAE(object):</span>
    <span class="s0">def </span><span class="s1">__init__(self</span><span class="s0">,</span>
                 <span class="s1">env</span><span class="s0">,</span>
                 <span class="s1">node_type</span><span class="s0">,</span>
                 <span class="s1">model_registrar</span><span class="s0">,</span>
                 <span class="s1">hyperparams</span><span class="s0">,</span>
                 <span class="s1">device</span><span class="s0">,</span>
                 <span class="s1">edge_types</span><span class="s0">,</span>
                 <span class="s1">log_writer=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s1">self.hyperparams = hyperparams</span>
        <span class="s1">self.env = env</span>
        <span class="s1">self.node_type = node_type</span>
        <span class="s1">self.model_registrar = model_registrar</span>
        <span class="s1">self.log_writer = log_writer</span>
        <span class="s1">self.device = device</span>
        <span class="s1">self.edge_types = [edge_type </span><span class="s0">for </span><span class="s1">edge_type </span><span class="s0">in </span><span class="s1">edge_types </span><span class="s0">if </span><span class="s1">edge_type[</span><span class="s2">0</span><span class="s1">] </span><span class="s0">is </span><span class="s1">node_type]</span>
        <span class="s1">self.curr_iter = </span><span class="s2">0</span>

        <span class="s1">self.node_modules = dict()</span>

        <span class="s1">self.min_hl = self.hyperparams[</span><span class="s3">'minimum_history_length'</span><span class="s1">]</span>
        <span class="s1">self.max_hl = self.hyperparams[</span><span class="s3">'maximum_history_length'</span><span class="s1">]</span>
        <span class="s1">self.ph = self.hyperparams[</span><span class="s3">'prediction_horizon'</span><span class="s1">]</span>
        <span class="s1">self.state = self.hyperparams[</span><span class="s3">'state'</span><span class="s1">]</span>
        <span class="s1">self.pred_state = self.hyperparams[</span><span class="s3">'pred_state'</span><span class="s1">][node_type]</span>
        <span class="s1">self.state_length = int(np.sum([len(entity_dims) </span><span class="s0">for </span><span class="s1">entity_dims </span><span class="s0">in </span><span class="s1">self.state[node_type].values()]))</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'incl_robot_node'</span><span class="s1">]:</span>
            <span class="s1">self.robot_state_length = int(</span>
                <span class="s1">np.sum([len(entity_dims) </span><span class="s0">for </span><span class="s1">entity_dims </span><span class="s0">in </span><span class="s1">self.state[env.robot_type].values()])</span>
            <span class="s1">)</span>
        <span class="s1">self.pred_state_length = int(np.sum([len(entity_dims) </span><span class="s0">for </span><span class="s1">entity_dims </span><span class="s0">in </span><span class="s1">self.pred_state.values()]))</span>

        <span class="s1">edge_types_str = [DirectedEdge.get_str_from_types(*edge_type) </span><span class="s0">for </span><span class="s1">edge_type </span><span class="s0">in </span><span class="s1">self.edge_types]</span>
        <span class="s1">self.create_graphical_model(edge_types_str)</span>

        <span class="s1">dynamic_class = getattr(dynamic_module</span><span class="s0">, </span><span class="s1">hyperparams[</span><span class="s3">'dynamic'</span><span class="s1">][self.node_type][</span><span class="s3">'name'</span><span class="s1">])</span>
        <span class="s1">dyn_limits = hyperparams[</span><span class="s3">'dynamic'</span><span class="s1">][self.node_type][</span><span class="s3">'limits'</span><span class="s1">]</span>
        <span class="s1">self.dynamic = dynamic_class(self.env.scenes[</span><span class="s2">0</span><span class="s1">].dt</span><span class="s0">, </span><span class="s1">dyn_limits</span><span class="s0">, </span><span class="s1">device</span><span class="s0">,</span>
                                     <span class="s1">self.model_registrar</span><span class="s0">, </span><span class="s1">self.x_size</span><span class="s0">, </span><span class="s1">self.node_type)</span>

    <span class="s0">def </span><span class="s1">set_curr_iter(self</span><span class="s0">, </span><span class="s1">curr_iter):</span>
        <span class="s1">self.curr_iter = curr_iter</span>

    <span class="s0">def </span><span class="s1">add_submodule(self</span><span class="s0">, </span><span class="s1">name</span><span class="s0">, </span><span class="s1">model_if_absent):</span>
        <span class="s1">self.node_modules[name] = self.model_registrar.get_model(name</span><span class="s0">, </span><span class="s1">model_if_absent)</span>

    <span class="s0">def </span><span class="s1">clear_submodules(self):</span>
        <span class="s1">self.node_modules.clear()</span>

    <span class="s0">def </span><span class="s1">create_node_models(self):</span>
        <span class="s4">############################</span>
        <span class="s4">#   Node History Encoder   #</span>
        <span class="s4">############################</span>
        <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/node_history_encoder'</span><span class="s0">,</span>
                           <span class="s1">model_if_absent=nn.LSTM(input_size=self.state_length</span><span class="s0">,</span>
                                                   <span class="s1">hidden_size=self.hyperparams[</span><span class="s3">'enc_rnn_dim_history'</span><span class="s1">]</span><span class="s0">,</span>
                                                   <span class="s1">batch_first=</span><span class="s0">True</span><span class="s1">))</span>

        <span class="s4">###########################</span>
        <span class="s4">#   Node Future Encoder   #</span>
        <span class="s4">###########################</span>
        <span class="s4"># We'll create this here, but then later check if in training mode.</span>
        <span class="s4"># Based on that, we'll factor this into the computation graph (or not).</span>
        <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/node_future_encoder'</span><span class="s0">,</span>
                           <span class="s1">model_if_absent=nn.LSTM(input_size=self.pred_state_length</span><span class="s0">,</span>
                                                   <span class="s1">hidden_size=self.hyperparams[</span><span class="s3">'enc_rnn_dim_future'</span><span class="s1">]</span><span class="s0">,</span>
                                                   <span class="s1">bidirectional=</span><span class="s0">True,</span>
                                                   <span class="s1">batch_first=</span><span class="s0">True</span><span class="s1">))</span>
        <span class="s4"># These are related to how you initialize states for the node future encoder.</span>
        <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/node_future_encoder/initial_h'</span><span class="s0">,</span>
                           <span class="s1">model_if_absent=nn.Linear(self.state_length</span><span class="s0">,</span>
                                                     <span class="s1">self.hyperparams[</span><span class="s3">'enc_rnn_dim_future'</span><span class="s1">]))</span>
        <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/node_future_encoder/initial_c'</span><span class="s0">,</span>
                           <span class="s1">model_if_absent=nn.Linear(self.state_length</span><span class="s0">,</span>
                                                     <span class="s1">self.hyperparams[</span><span class="s3">'enc_rnn_dim_future'</span><span class="s1">]))</span>

        <span class="s4">############################</span>
        <span class="s4">#   Robot Future Encoder   #</span>
        <span class="s4">############################</span>
        <span class="s4"># We'll create this here, but then later check if we're next to the robot.</span>
        <span class="s4"># Based on that, we'll factor this into the computation graph (or not).</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'incl_robot_node'</span><span class="s1">]:</span>
            <span class="s1">self.add_submodule(</span><span class="s3">'robot_future_encoder'</span><span class="s0">,</span>
                               <span class="s1">model_if_absent=nn.LSTM(input_size=self.robot_state_length</span><span class="s0">,</span>
                                                       <span class="s1">hidden_size=self.hyperparams[</span><span class="s3">'enc_rnn_dim_future'</span><span class="s1">]</span><span class="s0">,</span>
                                                       <span class="s1">bidirectional=</span><span class="s0">True,</span>
                                                       <span class="s1">batch_first=</span><span class="s0">True</span><span class="s1">))</span>
            <span class="s4"># These are related to how you initialize states for the robot future encoder.</span>
            <span class="s1">self.add_submodule(</span><span class="s3">'robot_future_encoder/initial_h'</span><span class="s0">,</span>
                               <span class="s1">model_if_absent=nn.Linear(self.robot_state_length</span><span class="s0">,</span>
                                                         <span class="s1">self.hyperparams[</span><span class="s3">'enc_rnn_dim_future'</span><span class="s1">]))</span>
            <span class="s1">self.add_submodule(</span><span class="s3">'robot_future_encoder/initial_c'</span><span class="s0">,</span>
                               <span class="s1">model_if_absent=nn.Linear(self.robot_state_length</span><span class="s0">,</span>
                                                         <span class="s1">self.hyperparams[</span><span class="s3">'enc_rnn_dim_future'</span><span class="s1">]))</span>

        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_encoding'</span><span class="s1">]:</span>
            <span class="s4">##############################</span>
            <span class="s4">#   Edge Influence Encoder   #</span>
            <span class="s4">##############################</span>
            <span class="s4"># NOTE: The edge influence encoding happens during calls</span>
            <span class="s4"># to forward or incremental_forward, so we don't create</span>
            <span class="s4"># a model for it here for the max and sum variants.</span>
            <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_influence_combine_method'</span><span class="s1">] == </span><span class="s3">'bi-rnn'</span><span class="s1">:</span>
                <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/edge_influence_encoder'</span><span class="s0">,</span>
                                   <span class="s1">model_if_absent=nn.LSTM(input_size=self.hyperparams[</span><span class="s3">'enc_rnn_dim_edge'</span><span class="s1">]</span><span class="s0">,</span>
                                                           <span class="s1">hidden_size=self.hyperparams[</span><span class="s3">'enc_rnn_dim_edge_influence'</span><span class="s1">]</span><span class="s0">,</span>
                                                           <span class="s1">bidirectional=</span><span class="s0">True,</span>
                                                           <span class="s1">batch_first=</span><span class="s0">True</span><span class="s1">))</span>

                <span class="s4"># Four times because we're trying to mimic a bi-directional</span>
                <span class="s4"># LSTM's output (which, here, is c and h from both ends).</span>
                <span class="s1">self.eie_output_dims = </span><span class="s2">4 </span><span class="s1">* self.hyperparams[</span><span class="s3">'enc_rnn_dim_edge_influence'</span><span class="s1">]</span>

            <span class="s0">elif </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_influence_combine_method'</span><span class="s1">] == </span><span class="s3">'attention'</span><span class="s1">:</span>
                <span class="s4"># Chose additive attention because of https://arxiv.org/pdf/1703.03906.pdf</span>
                <span class="s4"># We calculate an attention context vector using the encoded edges as the &quot;encoder&quot;</span>
                <span class="s4"># (that we attend _over_)</span>
                <span class="s4"># and the node history encoder representation as the &quot;decoder state&quot; (that we attend _on_).</span>
                <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/edge_influence_encoder'</span><span class="s0">,</span>
                                   <span class="s1">model_if_absent=AdditiveAttention(</span>
                                       <span class="s1">encoder_hidden_state_dim=self.hyperparams[</span><span class="s3">'enc_rnn_dim_edge_influence'</span><span class="s1">]</span><span class="s0">,</span>
                                       <span class="s1">decoder_hidden_state_dim=self.hyperparams[</span><span class="s3">'enc_rnn_dim_history'</span><span class="s1">]))</span>

                <span class="s1">self.eie_output_dims = self.hyperparams[</span><span class="s3">'enc_rnn_dim_edge_influence'</span><span class="s1">]</span>

        <span class="s4">###################</span>
        <span class="s4">#   Map Encoder   #</span>
        <span class="s4">###################</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'use_map_encoding'</span><span class="s1">]:</span>
            <span class="s0">if </span><span class="s1">self.node_type </span><span class="s0">in </span><span class="s1">self.hyperparams[</span><span class="s3">'map_encoder'</span><span class="s1">]:</span>
                <span class="s1">me_params = self.hyperparams[</span><span class="s3">'map_encoder'</span><span class="s1">][self.node_type]</span>
                <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/map_encoder'</span><span class="s0">,</span>
                                   <span class="s1">model_if_absent=CNNMapEncoder(me_params[</span><span class="s3">'map_channels'</span><span class="s1">]</span><span class="s0">,</span>
                                                                 <span class="s1">me_params[</span><span class="s3">'hidden_channels'</span><span class="s1">]</span><span class="s0">,</span>
                                                                 <span class="s1">me_params[</span><span class="s3">'output_size'</span><span class="s1">]</span><span class="s0">,</span>
                                                                 <span class="s1">me_params[</span><span class="s3">'masks'</span><span class="s1">]</span><span class="s0">,</span>
                                                                 <span class="s1">me_params[</span><span class="s3">'strides'</span><span class="s1">]</span><span class="s0">,</span>
                                                                 <span class="s1">me_params[</span><span class="s3">'patch_size'</span><span class="s1">]))</span>

        <span class="s4">################################</span>
        <span class="s4">#   Discrete Latent Variable   #</span>
        <span class="s4">################################</span>
        <span class="s1">self.latent = DiscreteLatent(self.hyperparams</span><span class="s0">, </span><span class="s1">self.device)</span>

        <span class="s4">######################################################################</span>
        <span class="s4">#   Various Fully-Connected Layers from Encoder to Latent Variable   #</span>
        <span class="s4">######################################################################</span>
        <span class="s4"># Node History Encoder</span>
        <span class="s1">x_size = self.hyperparams[</span><span class="s3">'enc_rnn_dim_history'</span><span class="s1">]</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_encoding'</span><span class="s1">]:</span>
            <span class="s4">#              Edge Encoder</span>
            <span class="s1">x_size += self.eie_output_dims</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'incl_robot_node'</span><span class="s1">]:</span>
            <span class="s4">#              Future Conditional Encoder</span>
            <span class="s1">x_size += </span><span class="s2">4 </span><span class="s1">* self.hyperparams[</span><span class="s3">'enc_rnn_dim_future'</span><span class="s1">]</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'use_map_encoding'</span><span class="s1">] </span><span class="s0">and </span><span class="s1">self.node_type </span><span class="s0">in </span><span class="s1">self.hyperparams[</span><span class="s3">'map_encoder'</span><span class="s1">]:</span>
            <span class="s4">#              Map Encoder</span>
            <span class="s1">x_size += self.hyperparams[</span><span class="s3">'map_encoder'</span><span class="s1">][self.node_type][</span><span class="s3">'output_size'</span><span class="s1">]</span>

        <span class="s1">z_size = self.hyperparams[</span><span class="s3">'N'</span><span class="s1">] * self.hyperparams[</span><span class="s3">'K'</span><span class="s1">]</span>

        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'p_z_x_MLP_dims'</span><span class="s1">] </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/p_z_x'</span><span class="s0">,</span>
                               <span class="s1">model_if_absent=nn.Linear(x_size</span><span class="s0">, </span><span class="s1">self.hyperparams[</span><span class="s3">'p_z_x_MLP_dims'</span><span class="s1">]))</span>
            <span class="s1">hx_size = self.hyperparams[</span><span class="s3">'p_z_x_MLP_dims'</span><span class="s1">]</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">hx_size = x_size</span>

        <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/hx_to_z'</span><span class="s0">,</span>
                           <span class="s1">model_if_absent=nn.Linear(hx_size</span><span class="s0">, </span><span class="s1">self.latent.z_dim))</span>

        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'q_z_xy_MLP_dims'</span><span class="s1">] </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/q_z_xy'</span><span class="s0">,</span>
                               <span class="s4">#                                           Node Future Encoder</span>
                               <span class="s1">model_if_absent=nn.Linear(x_size + </span><span class="s2">4 </span><span class="s1">* self.hyperparams[</span><span class="s3">'enc_rnn_dim_future'</span><span class="s1">]</span><span class="s0">,</span>
                                                         <span class="s1">self.hyperparams[</span><span class="s3">'q_z_xy_MLP_dims'</span><span class="s1">]))</span>
            <span class="s1">hxy_size = self.hyperparams[</span><span class="s3">'q_z_xy_MLP_dims'</span><span class="s1">]</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s4">#                           Node Future Encoder</span>
            <span class="s1">hxy_size = x_size + </span><span class="s2">4 </span><span class="s1">* self.hyperparams[</span><span class="s3">'enc_rnn_dim_future'</span><span class="s1">]</span>

        <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/hxy_to_z'</span><span class="s0">,</span>
                           <span class="s1">model_if_absent=nn.Linear(hxy_size</span><span class="s0">, </span><span class="s1">self.latent.z_dim))</span>

        <span class="s4">####################</span>
        <span class="s4">#   Decoder LSTM   #</span>
        <span class="s4">####################</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'incl_robot_node'</span><span class="s1">]:</span>
            <span class="s1">decoder_input_dims = self.pred_state_length + self.robot_state_length + z_size + x_size</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">decoder_input_dims = self.pred_state_length + z_size + x_size</span>

        <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/decoder/state_action'</span><span class="s0">,</span>
                           <span class="s1">model_if_absent=nn.Sequential(</span>
                               <span class="s1">nn.Linear(self.state_length</span><span class="s0">, </span><span class="s1">self.pred_state_length)))</span>

        <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/decoder/rnn_cell'</span><span class="s0">,</span>
                           <span class="s1">model_if_absent=nn.GRUCell(decoder_input_dims</span><span class="s0">, </span><span class="s1">self.hyperparams[</span><span class="s3">'dec_rnn_dim'</span><span class="s1">]))</span>
        <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/decoder/initial_h'</span><span class="s0">,</span>
                           <span class="s1">model_if_absent=nn.Linear(z_size + x_size</span><span class="s0">, </span><span class="s1">self.hyperparams[</span><span class="s3">'dec_rnn_dim'</span><span class="s1">]))</span>

        <span class="s4">###################</span>
        <span class="s4">#   Decoder GMM   #</span>
        <span class="s4">###################</span>
        <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/decoder/proj_to_GMM_log_pis'</span><span class="s0">,</span>
                           <span class="s1">model_if_absent=nn.Linear(self.hyperparams[</span><span class="s3">'dec_rnn_dim'</span><span class="s1">]</span><span class="s0">,</span>
                                                     <span class="s1">self.hyperparams[</span><span class="s3">'GMM_components'</span><span class="s1">]))</span>
        <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/decoder/proj_to_GMM_mus'</span><span class="s0">,</span>
                           <span class="s1">model_if_absent=nn.Linear(self.hyperparams[</span><span class="s3">'dec_rnn_dim'</span><span class="s1">]</span><span class="s0">,</span>
                                                     <span class="s1">self.hyperparams[</span><span class="s3">'GMM_components'</span><span class="s1">] * self.pred_state_length))</span>
        <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/decoder/proj_to_GMM_log_sigmas'</span><span class="s0">,</span>
                           <span class="s1">model_if_absent=nn.Linear(self.hyperparams[</span><span class="s3">'dec_rnn_dim'</span><span class="s1">]</span><span class="s0">,</span>
                                                     <span class="s1">self.hyperparams[</span><span class="s3">'GMM_components'</span><span class="s1">] * self.pred_state_length))</span>
        <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/decoder/proj_to_GMM_corrs'</span><span class="s0">,</span>
                           <span class="s1">model_if_absent=nn.Linear(self.hyperparams[</span><span class="s3">'dec_rnn_dim'</span><span class="s1">]</span><span class="s0">,</span>
                                                     <span class="s1">self.hyperparams[</span><span class="s3">'GMM_components'</span><span class="s1">]))</span>

        <span class="s1">self.x_size = x_size</span>
        <span class="s1">self.z_size = z_size</span>

    <span class="s0">def </span><span class="s1">create_edge_models(self</span><span class="s0">, </span><span class="s1">edge_types):</span>
        <span class="s0">for </span><span class="s1">edge_type </span><span class="s0">in </span><span class="s1">edge_types:</span>
            <span class="s1">neighbor_state_length = int(</span>
                <span class="s1">np.sum([len(entity_dims) </span><span class="s0">for </span><span class="s1">entity_dims </span><span class="s0">in </span><span class="s1">self.state[edge_type.split(</span><span class="s3">'-&gt;'</span><span class="s1">)[</span><span class="s2">1</span><span class="s1">]].values()]))</span>
            <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_state_combine_method'</span><span class="s1">] == </span><span class="s3">'pointnet'</span><span class="s1">:</span>
                <span class="s1">self.add_submodule(edge_type + </span><span class="s3">'/pointnet_encoder'</span><span class="s0">,</span>
                                   <span class="s1">model_if_absent=nn.Sequential(</span>
                                       <span class="s1">nn.Linear(self.state_length</span><span class="s0">, </span><span class="s2">2 </span><span class="s1">* self.state_length)</span><span class="s0">,</span>
                                       <span class="s1">nn.ReLU()</span><span class="s0">,</span>
                                       <span class="s1">nn.Linear(</span><span class="s2">2 </span><span class="s1">* self.state_length</span><span class="s0">, </span><span class="s2">2 </span><span class="s1">* self.state_length)</span><span class="s0">,</span>
                                       <span class="s1">nn.ReLU()))</span>

                <span class="s1">edge_encoder_input_size = </span><span class="s2">2 </span><span class="s1">* self.state_length + self.state_length</span>

            <span class="s0">elif </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_state_combine_method'</span><span class="s1">] == </span><span class="s3">'attention'</span><span class="s1">:</span>
                <span class="s1">self.add_submodule(self.node_type + </span><span class="s3">'/edge_attention_combine'</span><span class="s0">,</span>
                                   <span class="s1">model_if_absent=TemporallyBatchedAdditiveAttention(</span>
                                       <span class="s1">encoder_hidden_state_dim=self.state_length</span><span class="s0">,</span>
                                       <span class="s1">decoder_hidden_state_dim=self.state_length))</span>
                <span class="s1">edge_encoder_input_size = self.state_length + neighbor_state_length</span>

            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">edge_encoder_input_size = self.state_length + neighbor_state_length</span>

            <span class="s1">self.add_submodule(edge_type + </span><span class="s3">'/edge_encoder'</span><span class="s0">,</span>
                               <span class="s1">model_if_absent=nn.LSTM(input_size=edge_encoder_input_size</span><span class="s0">,</span>
                                                       <span class="s1">hidden_size=self.hyperparams[</span><span class="s3">'enc_rnn_dim_edge'</span><span class="s1">]</span><span class="s0">,</span>
                                                       <span class="s1">batch_first=</span><span class="s0">True</span><span class="s1">))</span>

    <span class="s0">def </span><span class="s1">create_graphical_model(self</span><span class="s0">, </span><span class="s1">edge_types):</span>
        <span class="s5">&quot;&quot;&quot; 
        Creates or queries all trainable components. 
 
        :param edge_types: List containing strings for all possible edge types for the node type. 
        :return: None 
        &quot;&quot;&quot;</span>
        <span class="s1">self.clear_submodules()</span>

        <span class="s4">############################</span>
        <span class="s4">#   Everything but Edges   #</span>
        <span class="s4">############################</span>
        <span class="s1">self.create_node_models()</span>

        <span class="s4">#####################</span>
        <span class="s4">#   Edge Encoders   #</span>
        <span class="s4">#####################</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_encoding'</span><span class="s1">]:</span>
            <span class="s1">self.create_edge_models(edge_types)</span>

        <span class="s0">for </span><span class="s1">name</span><span class="s0">, </span><span class="s1">module </span><span class="s0">in </span><span class="s1">self.node_modules.items():</span>
            <span class="s1">module.to(self.device)</span>

    <span class="s0">def </span><span class="s1">create_new_scheduler(self</span><span class="s0">, </span><span class="s1">name</span><span class="s0">, </span><span class="s1">annealer</span><span class="s0">, </span><span class="s1">annealer_kws</span><span class="s0">, </span><span class="s1">creation_condition=</span><span class="s0">True</span><span class="s1">):</span>
        <span class="s1">value_scheduler = </span><span class="s0">None</span>
        <span class="s1">rsetattr(self</span><span class="s0">, </span><span class="s1">name + </span><span class="s3">'_scheduler'</span><span class="s0">, </span><span class="s1">value_scheduler)</span>
        <span class="s0">if </span><span class="s1">creation_condition:</span>
            <span class="s1">annealer_kws[</span><span class="s3">'device'</span><span class="s1">] = self.device</span>
            <span class="s1">value_annealer = annealer(annealer_kws)</span>
            <span class="s1">rsetattr(self</span><span class="s0">, </span><span class="s1">name + </span><span class="s3">'_annealer'</span><span class="s0">, </span><span class="s1">value_annealer)</span>

            <span class="s4"># This is the value that we'll update on each call of</span>
            <span class="s4"># step_annealers().</span>
            <span class="s1">rsetattr(self</span><span class="s0">, </span><span class="s1">name</span><span class="s0">, </span><span class="s1">value_annealer(</span><span class="s2">0</span><span class="s1">).clone().detach())</span>
            <span class="s1">dummy_optimizer = optim.Optimizer([rgetattr(self</span><span class="s0">, </span><span class="s1">name)]</span><span class="s0">, </span><span class="s1">{</span><span class="s3">'lr'</span><span class="s1">: value_annealer(</span><span class="s2">0</span><span class="s1">).clone().detach()})</span>
            <span class="s1">rsetattr(self</span><span class="s0">, </span><span class="s1">name + </span><span class="s3">'_optimizer'</span><span class="s0">, </span><span class="s1">dummy_optimizer)</span>

            <span class="s1">value_scheduler = CustomLR(dummy_optimizer</span><span class="s0">,</span>
                                       <span class="s1">value_annealer)</span>
            <span class="s1">rsetattr(self</span><span class="s0">, </span><span class="s1">name + </span><span class="s3">'_scheduler'</span><span class="s0">, </span><span class="s1">value_scheduler)</span>

        <span class="s1">self.schedulers.append(value_scheduler)</span>
        <span class="s1">self.annealed_vars.append(name)</span>

    <span class="s0">def </span><span class="s1">set_annealing_params(self):</span>
        <span class="s1">self.schedulers = list()</span>
        <span class="s1">self.annealed_vars = list()</span>

        <span class="s1">self.create_new_scheduler(name=</span><span class="s3">'kl_weight'</span><span class="s0">,</span>
                                  <span class="s1">annealer=sigmoid_anneal</span><span class="s0">,</span>
                                  <span class="s1">annealer_kws={</span>
                                      <span class="s3">'start'</span><span class="s1">: self.hyperparams[</span><span class="s3">'kl_weight_start'</span><span class="s1">]</span><span class="s0">,</span>
                                      <span class="s3">'finish'</span><span class="s1">: self.hyperparams[</span><span class="s3">'kl_weight'</span><span class="s1">]</span><span class="s0">,</span>
                                      <span class="s3">'center_step'</span><span class="s1">: self.hyperparams[</span><span class="s3">'kl_crossover'</span><span class="s1">]</span><span class="s0">,</span>
                                      <span class="s3">'steps_lo_to_hi'</span><span class="s1">: self.hyperparams[</span><span class="s3">'kl_crossover'</span><span class="s1">] / self.hyperparams[</span>
                                          <span class="s3">'kl_sigmoid_divisor'</span><span class="s1">]</span>
                                  <span class="s1">})</span>

        <span class="s1">self.create_new_scheduler(name=</span><span class="s3">'latent.temp'</span><span class="s0">,</span>
                                  <span class="s1">annealer=exp_anneal</span><span class="s0">,</span>
                                  <span class="s1">annealer_kws={</span>
                                      <span class="s3">'start'</span><span class="s1">: self.hyperparams[</span><span class="s3">'tau_init'</span><span class="s1">]</span><span class="s0">,</span>
                                      <span class="s3">'finish'</span><span class="s1">: self.hyperparams[</span><span class="s3">'tau_final'</span><span class="s1">]</span><span class="s0">,</span>
                                      <span class="s3">'rate'</span><span class="s1">: self.hyperparams[</span><span class="s3">'tau_decay_rate'</span><span class="s1">]</span>
                                  <span class="s1">})</span>

        <span class="s1">self.create_new_scheduler(name=</span><span class="s3">'latent.z_logit_clip'</span><span class="s0">,</span>
                                  <span class="s1">annealer=sigmoid_anneal</span><span class="s0">,</span>
                                  <span class="s1">annealer_kws={</span>
                                      <span class="s3">'start'</span><span class="s1">: self.hyperparams[</span><span class="s3">'z_logit_clip_start'</span><span class="s1">]</span><span class="s0">,</span>
                                      <span class="s3">'finish'</span><span class="s1">: self.hyperparams[</span><span class="s3">'z_logit_clip_final'</span><span class="s1">]</span><span class="s0">,</span>
                                      <span class="s3">'center_step'</span><span class="s1">: self.hyperparams[</span><span class="s3">'z_logit_clip_crossover'</span><span class="s1">]</span><span class="s0">,</span>
                                      <span class="s3">'steps_lo_to_hi'</span><span class="s1">: self.hyperparams[</span><span class="s3">'z_logit_clip_crossover'</span><span class="s1">] / self.hyperparams[</span>
                                          <span class="s3">'z_logit_clip_divisor'</span><span class="s1">]</span>
                                  <span class="s1">}</span><span class="s0">,</span>
                                  <span class="s1">creation_condition=self.hyperparams[</span><span class="s3">'use_z_logit_clipping'</span><span class="s1">])</span>

    <span class="s0">def </span><span class="s1">step_annealers(self):</span>
        <span class="s4"># This should manage all of the step-wise changed</span>
        <span class="s4"># parameters automatically.</span>
        <span class="s0">for </span><span class="s1">idx</span><span class="s0">, </span><span class="s1">annealed_var </span><span class="s0">in </span><span class="s1">enumerate(self.annealed_vars):</span>
            <span class="s0">if </span><span class="s1">rgetattr(self</span><span class="s0">, </span><span class="s1">annealed_var + </span><span class="s3">'_scheduler'</span><span class="s1">) </span><span class="s0">is not None</span><span class="s1">:</span>
                <span class="s4"># First we step the scheduler.</span>
                <span class="s0">with </span><span class="s1">warnings.catch_warnings():  </span><span class="s4"># We use a dummy optimizer: Warning because no .step() was called on it</span>
                    <span class="s1">warnings.simplefilter(</span><span class="s3">&quot;ignore&quot;</span><span class="s1">)</span>
                    <span class="s1">rgetattr(self</span><span class="s0">, </span><span class="s1">annealed_var + </span><span class="s3">'_scheduler'</span><span class="s1">).step()</span>

                <span class="s4"># Then we set the annealed vars' value.</span>
                <span class="s1">rsetattr(self</span><span class="s0">, </span><span class="s1">annealed_var</span><span class="s0">, </span><span class="s1">rgetattr(self</span><span class="s0">, </span><span class="s1">annealed_var + </span><span class="s3">'_optimizer'</span><span class="s1">).param_groups[</span><span class="s2">0</span><span class="s1">][</span><span class="s3">'lr'</span><span class="s1">])</span>

        <span class="s1">self.summarize_annealers()</span>

    <span class="s0">def </span><span class="s1">summarize_annealers(self):</span>
        <span class="s0">if </span><span class="s1">self.log_writer </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s0">for </span><span class="s1">annealed_var </span><span class="s0">in </span><span class="s1">self.annealed_vars:</span>
                <span class="s0">if </span><span class="s1">rgetattr(self</span><span class="s0">, </span><span class="s1">annealed_var) </span><span class="s0">is not None</span><span class="s1">:</span>
                    <span class="s1">self.log_writer.add_scalar(</span><span class="s3">'%s/%s' </span><span class="s1">% (str(self.node_type)</span><span class="s0">, </span><span class="s1">annealed_var.replace(</span><span class="s3">'.'</span><span class="s0">, </span><span class="s3">'/'</span><span class="s1">))</span><span class="s0">,</span>
                                               <span class="s1">rgetattr(self</span><span class="s0">, </span><span class="s1">annealed_var)</span><span class="s0">, </span><span class="s1">self.curr_iter)</span>

    <span class="s0">def </span><span class="s1">obtain_encoded_tensors(self</span><span class="s0">,</span>
                               <span class="s1">mode</span><span class="s0">,</span>
                               <span class="s1">inputs</span><span class="s0">,</span>
                               <span class="s1">inputs_st</span><span class="s0">,</span>
                               <span class="s1">labels</span><span class="s0">,</span>
                               <span class="s1">labels_st</span><span class="s0">,</span>
                               <span class="s1">first_history_indices</span><span class="s0">,</span>
                               <span class="s1">neighbors</span><span class="s0">,</span>
                               <span class="s1">neighbors_edge_value</span><span class="s0">,</span>
                               <span class="s1">robot</span><span class="s0">,</span>
                               <span class="s1">map) -&gt; (torch.Tensor</span><span class="s0">,</span>
                                        <span class="s1">torch.Tensor</span><span class="s0">,</span>
                                        <span class="s1">torch.Tensor</span><span class="s0">,</span>
                                        <span class="s1">torch.Tensor</span><span class="s0">,</span>
                                        <span class="s1">torch.Tensor</span><span class="s0">,</span>
                                        <span class="s1">torch.Tensor):</span>
        <span class="s5">&quot;&quot;&quot; 
        Encodes input and output tensors for node and robot. 
 
        :param mode: Mode in which the model is operated. E.g. Train, Eval, Predict. 
        :param inputs: Input tensor including the state for each agent over time [bs, t, state]. 
        :param inputs_st: Standardized input tensor. 
        :param labels: Label tensor including the label output for each agent over time [bs, t, pred_state]. 
        :param labels_st: Standardized label tensor. 
        :param first_history_indices: First timestep (index) in scene for which data is available for a node [bs] 
        :param neighbors: Preprocessed dict (indexed by edge type) of list of neighbor states over time. 
                            [[bs, t, neighbor state]] 
        :param neighbors_edge_value: Preprocessed edge values for all neighbor nodes [[N]] 
        :param robot: Standardized robot state over time. [bs, t, robot_state] 
        :param map: Tensor of Map information. [bs, channels, x, y] 
        :return: tuple(x, x_nr_t, y_e, y_r, y, n_s_t0) 
            WHERE 
            - x: Encoded input / condition tensor to the CVAE x_e. 
            - x_r_t: Robot state (if robot is in scene). 
            - y_e: Encoded label / future of the node. 
            - y_r: Encoded future of the robot. 
            - y: Label / future of the node. 
            - n_s_t0: Standardized current state of the node. 
        &quot;&quot;&quot;</span>

        <span class="s1">x</span><span class="s0">, </span><span class="s1">x_r_t</span><span class="s0">, </span><span class="s1">y_e</span><span class="s0">, </span><span class="s1">y_r</span><span class="s0">, </span><span class="s1">y = </span><span class="s0">None, None, None, None, None</span>
        <span class="s1">initial_dynamics = dict()</span>

        <span class="s1">batch_size = inputs.shape[</span><span class="s2">0</span><span class="s1">]</span>

        <span class="s4">#########################################</span>
        <span class="s4"># Provide basic information to encoders #</span>
        <span class="s4">#########################################</span>
        <span class="s1">node_history = inputs</span>
        <span class="s1">node_present_state = inputs[:</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s1">]</span>
        <span class="s1">node_pos = inputs[:</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">:</span><span class="s2">2</span><span class="s1">]</span>
        <span class="s1">node_vel = inputs[:</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">:</span><span class="s2">4</span><span class="s1">]</span>

        <span class="s1">node_history_st = inputs_st</span>
        <span class="s1">node_present_state_st = inputs_st[:</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s1">]</span>
        <span class="s1">node_pos_st = inputs_st[:</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">0</span><span class="s1">:</span><span class="s2">2</span><span class="s1">]</span>
        <span class="s1">node_vel_st = inputs_st[:</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span><span class="s1">:</span><span class="s2">4</span><span class="s1">]</span>

        <span class="s1">n_s_t0 = node_present_state_st</span>

        <span class="s1">initial_dynamics[</span><span class="s3">'pos'</span><span class="s1">] = node_pos</span>
        <span class="s1">initial_dynamics[</span><span class="s3">'vel'</span><span class="s1">] = node_vel</span>

        <span class="s1">self.dynamic.set_initial_condition(initial_dynamics)</span>

        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'incl_robot_node'</span><span class="s1">]:</span>
            <span class="s1">x_r_t</span><span class="s0">, </span><span class="s1">y_r = robot[...</span><span class="s0">, </span><span class="s2">0</span><span class="s0">, </span><span class="s1">:]</span><span class="s0">, </span><span class="s1">robot[...</span><span class="s0">, </span><span class="s2">1</span><span class="s1">:</span><span class="s0">, </span><span class="s1">:]</span>

        <span class="s4">##################</span>
        <span class="s4"># Encode History #</span>
        <span class="s4">##################</span>
        <span class="s1">node_history_encoded = self.encode_node_history(mode</span><span class="s0">,</span>
                                                        <span class="s1">node_history_st</span><span class="s0">,</span>
                                                        <span class="s1">first_history_indices)</span>

        <span class="s4">##################</span>
        <span class="s4"># Encode Present #</span>
        <span class="s4">##################</span>
        <span class="s1">node_present = node_present_state_st  </span><span class="s4"># [bs, state_dim]</span>

        <span class="s4">##################</span>
        <span class="s4"># Encode Future #</span>
        <span class="s4">##################</span>
        <span class="s0">if </span><span class="s1">mode != ModeKeys.PREDICT:</span>
            <span class="s1">y = labels_st</span>

        <span class="s4">##############################</span>
        <span class="s4"># Encode Node Edges per Type #</span>
        <span class="s4">##############################</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_encoding'</span><span class="s1">]:</span>
            <span class="s1">node_edges_encoded = list()</span>
            <span class="s0">for </span><span class="s1">edge_type </span><span class="s0">in </span><span class="s1">self.edge_types:</span>
                <span class="s4"># Encode edges for given edge type</span>
                <span class="s1">encoded_edges_type = self.encode_edge(mode</span><span class="s0">,</span>
                                                      <span class="s1">node_history</span><span class="s0">,</span>
                                                      <span class="s1">node_history_st</span><span class="s0">,</span>
                                                      <span class="s1">edge_type</span><span class="s0">,</span>
                                                      <span class="s1">neighbors[edge_type]</span><span class="s0">,</span>
                                                      <span class="s1">neighbors_edge_value[edge_type]</span><span class="s0">,</span>
                                                      <span class="s1">first_history_indices)</span>
                <span class="s1">node_edges_encoded.append(encoded_edges_type)  </span><span class="s4"># List of [bs/nbs, enc_rnn_dim]</span>
            <span class="s4">#####################</span>
            <span class="s4"># Encode Node Edges #</span>
            <span class="s4">#####################</span>
            <span class="s1">total_edge_influence = self.encode_total_edge_influence(mode</span><span class="s0">,</span>
                                                                    <span class="s1">node_edges_encoded</span><span class="s0">,</span>
                                                                    <span class="s1">node_history_encoded</span><span class="s0">,</span>
                                                                    <span class="s1">batch_size)</span>

        <span class="s4">################</span>
        <span class="s4"># Map Encoding #</span>
        <span class="s4">################</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'use_map_encoding'</span><span class="s1">] </span><span class="s0">and </span><span class="s1">self.node_type </span><span class="s0">in </span><span class="s1">self.hyperparams[</span><span class="s3">'map_encoder'</span><span class="s1">]:</span>
            <span class="s0">if </span><span class="s1">self.log_writer </span><span class="s0">and </span><span class="s1">(self.curr_iter + </span><span class="s2">1</span><span class="s1">) % </span><span class="s2">500 </span><span class="s1">== </span><span class="s2">0</span><span class="s1">:</span>
                <span class="s1">map_clone = map.clone()</span>
                <span class="s1">map_patch = self.hyperparams[</span><span class="s3">'map_encoder'</span><span class="s1">][self.node_type][</span><span class="s3">'patch_size'</span><span class="s1">]</span>
                <span class="s1">map_clone[:</span><span class="s0">, </span><span class="s1">:</span><span class="s0">, </span><span class="s1">map_patch[</span><span class="s2">1</span><span class="s1">] - </span><span class="s2">5</span><span class="s1">:map_patch[</span><span class="s2">1</span><span class="s1">] + </span><span class="s2">5</span><span class="s0">, </span><span class="s1">map_patch[</span><span class="s2">0</span><span class="s1">] - </span><span class="s2">5</span><span class="s1">:map_patch[</span><span class="s2">0</span><span class="s1">] + </span><span class="s2">5</span><span class="s1">] = </span><span class="s2">1.</span>
                <span class="s1">self.log_writer.add_images(</span><span class="s3">f&quot;</span><span class="s0">{</span><span class="s1">self.node_type</span><span class="s0">}</span><span class="s3">/cropped_maps&quot;</span><span class="s0">, </span><span class="s1">map_clone</span><span class="s0">,</span>
                                           <span class="s1">self.curr_iter</span><span class="s0">, </span><span class="s1">dataformats=</span><span class="s3">'NCWH'</span><span class="s1">)</span>

            <span class="s1">encoded_map = self.node_modules[self.node_type + </span><span class="s3">'/map_encoder'</span><span class="s1">](map * </span><span class="s2">2. </span><span class="s1">- </span><span class="s2">1.</span><span class="s0">, </span><span class="s1">(mode == ModeKeys.TRAIN))</span>
            <span class="s1">do = self.hyperparams[</span><span class="s3">'map_encoder'</span><span class="s1">][self.node_type][</span><span class="s3">'dropout'</span><span class="s1">]</span>
            <span class="s1">encoded_map = F.dropout(encoded_map</span><span class="s0">, </span><span class="s1">do</span><span class="s0">, </span><span class="s1">training=(mode == ModeKeys.TRAIN))</span>

        <span class="s4">######################################</span>
        <span class="s4"># Concatenate Encoder Outputs into x #</span>
        <span class="s4">######################################</span>
        <span class="s1">x_concat_list = list()</span>

        <span class="s4"># Every node has an edge-influence encoder (which could just be zero).</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_encoding'</span><span class="s1">]:</span>
            <span class="s1">x_concat_list.append(total_edge_influence)  </span><span class="s4"># [bs/nbs, 4*enc_rnn_dim]</span>

        <span class="s4"># Every node has a history encoder.</span>
        <span class="s1">x_concat_list.append(node_history_encoded)  </span><span class="s4"># [bs/nbs, enc_rnn_dim_history]</span>

        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'incl_robot_node'</span><span class="s1">]:</span>
            <span class="s1">robot_future_encoder = self.encode_robot_future(mode</span><span class="s0">, </span><span class="s1">x_r_t</span><span class="s0">, </span><span class="s1">y_r)</span>
            <span class="s1">x_concat_list.append(robot_future_encoder)</span>

        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'use_map_encoding'</span><span class="s1">] </span><span class="s0">and </span><span class="s1">self.node_type </span><span class="s0">in </span><span class="s1">self.hyperparams[</span><span class="s3">'map_encoder'</span><span class="s1">]:</span>
            <span class="s0">if </span><span class="s1">self.log_writer:</span>
                <span class="s1">self.log_writer.add_scalar(</span><span class="s3">f&quot;</span><span class="s0">{</span><span class="s1">self.node_type</span><span class="s0">}</span><span class="s3">/encoded_map_max&quot;</span><span class="s0">,</span>
                                           <span class="s1">torch.max(torch.abs(encoded_map))</span><span class="s0">, </span><span class="s1">self.curr_iter)</span>
            <span class="s1">x_concat_list.append(encoded_map)</span>

        <span class="s1">x = torch.cat(x_concat_list</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">1</span><span class="s1">)</span>

        <span class="s0">if </span><span class="s1">mode == ModeKeys.TRAIN </span><span class="s0">or </span><span class="s1">mode == ModeKeys.EVAL:</span>
            <span class="s1">y_e = self.encode_node_future(mode</span><span class="s0">, </span><span class="s1">node_present</span><span class="s0">, </span><span class="s1">y)</span>

        <span class="s0">return </span><span class="s1">x</span><span class="s0">, </span><span class="s1">x_r_t</span><span class="s0">, </span><span class="s1">y_e</span><span class="s0">, </span><span class="s1">y_r</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">n_s_t0</span>

    <span class="s0">def </span><span class="s1">encode_node_history(self</span><span class="s0">, </span><span class="s1">mode</span><span class="s0">, </span><span class="s1">node_hist</span><span class="s0">, </span><span class="s1">first_history_indices):</span>
        <span class="s5">&quot;&quot;&quot; 
        Encodes the nodes history. 
 
        :param mode: Mode in which the model is operated. E.g. Train, Eval, Predict. 
        :param node_hist: Historic and current state of the node. [bs, mhl, state] 
        :param first_history_indices: First timestep (index) in scene for which data is available for a node [bs] 
        :return: Encoded node history tensor. [bs, enc_rnn_dim] 
        &quot;&quot;&quot;</span>
        <span class="s1">outputs</span><span class="s0">, </span><span class="s1">_ = run_lstm_on_variable_length_seqs(self.node_modules[self.node_type + </span><span class="s3">'/node_history_encoder'</span><span class="s1">]</span><span class="s0">,</span>
                                                      <span class="s1">original_seqs=node_hist</span><span class="s0">,</span>
                                                      <span class="s1">lower_indices=first_history_indices)</span>

        <span class="s1">outputs = F.dropout(outputs</span><span class="s0">,</span>
                            <span class="s1">p=</span><span class="s2">1. </span><span class="s1">- self.hyperparams[</span><span class="s3">'rnn_kwargs'</span><span class="s1">][</span><span class="s3">'dropout_keep_prob'</span><span class="s1">]</span><span class="s0">,</span>
                            <span class="s1">training=(mode == ModeKeys.TRAIN))  </span><span class="s4"># [bs, max_time, enc_rnn_dim]</span>

        <span class="s1">last_index_per_sequence = -(first_history_indices + </span><span class="s2">1</span><span class="s1">)</span>

        <span class="s0">return </span><span class="s1">outputs[torch.arange(first_history_indices.shape[</span><span class="s2">0</span><span class="s1">])</span><span class="s0">, </span><span class="s1">last_index_per_sequence]</span>

    <span class="s0">def </span><span class="s1">encode_edge(self</span><span class="s0">,</span>
                    <span class="s1">mode</span><span class="s0">,</span>
                    <span class="s1">node_history</span><span class="s0">,</span>
                    <span class="s1">node_history_st</span><span class="s0">,</span>
                    <span class="s1">edge_type</span><span class="s0">,</span>
                    <span class="s1">neighbors</span><span class="s0">,</span>
                    <span class="s1">neighbors_edge_value</span><span class="s0">,</span>
                    <span class="s1">first_history_indices):</span>

        <span class="s1">max_hl = self.hyperparams[</span><span class="s3">'maximum_history_length'</span><span class="s1">]</span>

        <span class="s1">edge_states_list = list()  </span><span class="s4"># list of [#of neighbors, max_ht, state_dim]</span>
        <span class="s0">for </span><span class="s1">i</span><span class="s0">, </span><span class="s1">neighbor_states </span><span class="s0">in </span><span class="s1">enumerate(neighbors):  </span><span class="s4"># Get neighbors for timestep in batch</span>
            <span class="s0">if </span><span class="s1">len(neighbor_states) == </span><span class="s2">0</span><span class="s1">:  </span><span class="s4"># There are no neighbors for edge type # TODO necessary?</span>
                <span class="s1">neighbor_state_length = int(</span>
                    <span class="s1">np.sum([len(entity_dims) </span><span class="s0">for </span><span class="s1">entity_dims </span><span class="s0">in </span><span class="s1">self.state[edge_type[</span><span class="s2">1</span><span class="s1">]].values()])</span>
                <span class="s1">)</span>
                <span class="s1">edge_states_list.append(torch.zeros((</span><span class="s2">1</span><span class="s0">, </span><span class="s1">max_hl + </span><span class="s2">1</span><span class="s0">, </span><span class="s1">neighbor_state_length)</span><span class="s0">, </span><span class="s1">device=self.device))</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">edge_states_list.append(torch.stack(neighbor_states</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">).to(self.device))</span>

        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_state_combine_method'</span><span class="s1">] == </span><span class="s3">'sum'</span><span class="s1">:</span>
            <span class="s4"># Used in Structural-RNN to combine edges as well.</span>
            <span class="s1">op_applied_edge_states_list = list()</span>
            <span class="s0">for </span><span class="s1">neighbors_state </span><span class="s0">in </span><span class="s1">edge_states_list:</span>
                <span class="s1">op_applied_edge_states_list.append(torch.sum(neighbors_state</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">))</span>
            <span class="s1">combined_neighbors = torch.stack(op_applied_edge_states_list</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>
            <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'dynamic_edges'</span><span class="s1">] == </span><span class="s3">'yes'</span><span class="s1">:</span>
                <span class="s4"># Should now be (bs, time, 1)</span>
                <span class="s1">op_applied_edge_mask_list = list()</span>
                <span class="s0">for </span><span class="s1">edge_value </span><span class="s0">in </span><span class="s1">neighbors_edge_value:</span>
                    <span class="s1">op_applied_edge_mask_list.append(torch.clamp(torch.sum(edge_value.to(self.device)</span><span class="s0">,</span>
                                                                           <span class="s1">dim=</span><span class="s2">0</span><span class="s0">, </span><span class="s1">keepdim=</span><span class="s0">True</span><span class="s1">)</span><span class="s0">, </span><span class="s1">max=</span><span class="s2">1.</span><span class="s1">))</span>
                <span class="s1">combined_edge_masks = torch.stack(op_applied_edge_mask_list</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>

        <span class="s0">elif </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_state_combine_method'</span><span class="s1">] == </span><span class="s3">'max'</span><span class="s1">:</span>
            <span class="s4"># Used in NLP, e.g. max over word embeddings in a sentence.</span>
            <span class="s1">op_applied_edge_states_list = list()</span>
            <span class="s0">for </span><span class="s1">neighbors_state </span><span class="s0">in </span><span class="s1">edge_states_list:</span>
                <span class="s1">op_applied_edge_states_list.append(torch.max(neighbors_state</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">))</span>
            <span class="s1">combined_neighbors = torch.stack(op_applied_edge_states_list</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>
            <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'dynamic_edges'</span><span class="s1">] == </span><span class="s3">'yes'</span><span class="s1">:</span>
                <span class="s4"># Should now be (bs, time, 1)</span>
                <span class="s1">op_applied_edge_mask_list = list()</span>
                <span class="s0">for </span><span class="s1">edge_value </span><span class="s0">in </span><span class="s1">neighbors_edge_value:</span>
                    <span class="s1">op_applied_edge_mask_list.append(torch.clamp(torch.max(edge_value.to(self.device)</span><span class="s0">,</span>
                                                                           <span class="s1">dim=</span><span class="s2">0</span><span class="s0">, </span><span class="s1">keepdim=</span><span class="s0">True</span><span class="s1">)</span><span class="s0">, </span><span class="s1">max=</span><span class="s2">1.</span><span class="s1">))</span>
                <span class="s1">combined_edge_masks = torch.stack(op_applied_edge_mask_list</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>

        <span class="s0">elif </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_state_combine_method'</span><span class="s1">] == </span><span class="s3">'mean'</span><span class="s1">:</span>
            <span class="s4"># Used in NLP, e.g. mean over word embeddings in a sentence.</span>
            <span class="s1">op_applied_edge_states_list = list()</span>
            <span class="s0">for </span><span class="s1">neighbors_state </span><span class="s0">in </span><span class="s1">edge_states_list:</span>
                <span class="s1">op_applied_edge_states_list.append(torch.mean(neighbors_state</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">))</span>
            <span class="s1">combined_neighbors = torch.stack(op_applied_edge_states_list</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>
            <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'dynamic_edges'</span><span class="s1">] == </span><span class="s3">'yes'</span><span class="s1">:</span>
                <span class="s4"># Should now be (bs, time, 1)</span>
                <span class="s1">op_applied_edge_mask_list = list()</span>
                <span class="s0">for </span><span class="s1">edge_value </span><span class="s0">in </span><span class="s1">neighbors_edge_value:</span>
                    <span class="s1">op_applied_edge_mask_list.append(torch.clamp(torch.mean(edge_value.to(self.device)</span><span class="s0">,</span>
                                                                            <span class="s1">dim=</span><span class="s2">0</span><span class="s0">, </span><span class="s1">keepdim=</span><span class="s0">True</span><span class="s1">)</span><span class="s0">, </span><span class="s1">max=</span><span class="s2">1.</span><span class="s1">))</span>
                <span class="s1">combined_edge_masks = torch.stack(op_applied_edge_mask_list</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>

        <span class="s1">joint_history = torch.cat([combined_neighbors</span><span class="s0">, </span><span class="s1">node_history_st]</span><span class="s0">, </span><span class="s1">dim=-</span><span class="s2">1</span><span class="s1">)</span>

        <span class="s1">outputs</span><span class="s0">, </span><span class="s1">_ = run_lstm_on_variable_length_seqs(</span>
            <span class="s1">self.node_modules[DirectedEdge.get_str_from_types(*edge_type) + </span><span class="s3">'/edge_encoder'</span><span class="s1">]</span><span class="s0">,</span>
            <span class="s1">original_seqs=joint_history</span><span class="s0">,</span>
            <span class="s1">lower_indices=first_history_indices</span>
        <span class="s1">)</span>

        <span class="s1">outputs = F.dropout(outputs</span><span class="s0">,</span>
                            <span class="s1">p=</span><span class="s2">1. </span><span class="s1">- self.hyperparams[</span><span class="s3">'rnn_kwargs'</span><span class="s1">][</span><span class="s3">'dropout_keep_prob'</span><span class="s1">]</span><span class="s0">,</span>
                            <span class="s1">training=(mode == ModeKeys.TRAIN))  </span><span class="s4"># [bs, max_time, enc_rnn_dim]</span>

        <span class="s1">last_index_per_sequence = -(first_history_indices + </span><span class="s2">1</span><span class="s1">)</span>
        <span class="s1">ret = outputs[torch.arange(last_index_per_sequence.shape[</span><span class="s2">0</span><span class="s1">])</span><span class="s0">, </span><span class="s1">last_index_per_sequence]</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'dynamic_edges'</span><span class="s1">] == </span><span class="s3">'yes'</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">ret * combined_edge_masks</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">ret</span>

    <span class="s0">def </span><span class="s1">encode_total_edge_influence(self</span><span class="s0">, </span><span class="s1">mode</span><span class="s0">, </span><span class="s1">encoded_edges</span><span class="s0">, </span><span class="s1">node_history_encoder</span><span class="s0">, </span><span class="s1">batch_size):</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_influence_combine_method'</span><span class="s1">] == </span><span class="s3">'sum'</span><span class="s1">:</span>
            <span class="s1">stacked_encoded_edges = torch.stack(encoded_edges</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>
            <span class="s1">combined_edges = torch.sum(stacked_encoded_edges</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>

        <span class="s0">elif </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_influence_combine_method'</span><span class="s1">] == </span><span class="s3">'mean'</span><span class="s1">:</span>
            <span class="s1">stacked_encoded_edges = torch.stack(encoded_edges</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>
            <span class="s1">combined_edges = torch.mean(stacked_encoded_edges</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>

        <span class="s0">elif </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_influence_combine_method'</span><span class="s1">] == </span><span class="s3">'max'</span><span class="s1">:</span>
            <span class="s1">stacked_encoded_edges = torch.stack(encoded_edges</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>
            <span class="s1">combined_edges = torch.max(stacked_encoded_edges</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>

        <span class="s0">elif </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_influence_combine_method'</span><span class="s1">] == </span><span class="s3">'bi-rnn'</span><span class="s1">:</span>
            <span class="s0">if </span><span class="s1">len(encoded_edges) == </span><span class="s2">0</span><span class="s1">:</span>
                <span class="s1">combined_edges = torch.zeros((batch_size</span><span class="s0">, </span><span class="s1">self.eie_output_dims)</span><span class="s0">, </span><span class="s1">device=self.device)</span>

            <span class="s0">else</span><span class="s1">:</span>
                <span class="s4"># axis=1 because then we get size [batch_size, max_time, depth]</span>
                <span class="s1">encoded_edges = torch.stack(encoded_edges</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">1</span><span class="s1">)</span>

                <span class="s1">_</span><span class="s0">, </span><span class="s1">state = self.node_modules[self.node_type + </span><span class="s3">'/edge_influence_encoder'</span><span class="s1">](encoded_edges)</span>
                <span class="s1">combined_edges = unpack_RNN_state(state)</span>
                <span class="s1">combined_edges = F.dropout(combined_edges</span><span class="s0">,</span>
                                           <span class="s1">p=</span><span class="s2">1. </span><span class="s1">- self.hyperparams[</span><span class="s3">'rnn_kwargs'</span><span class="s1">][</span><span class="s3">'dropout_keep_prob'</span><span class="s1">]</span><span class="s0">,</span>
                                           <span class="s1">training=(mode == ModeKeys.TRAIN))</span>

        <span class="s0">elif </span><span class="s1">self.hyperparams[</span><span class="s3">'edge_influence_combine_method'</span><span class="s1">] == </span><span class="s3">'attention'</span><span class="s1">:</span>
            <span class="s4"># Used in Social Attention (https://arxiv.org/abs/1710.04689)</span>
            <span class="s0">if </span><span class="s1">len(encoded_edges) == </span><span class="s2">0</span><span class="s1">:</span>
                <span class="s1">combined_edges = torch.zeros((batch_size</span><span class="s0">, </span><span class="s1">self.eie_output_dims)</span><span class="s0">, </span><span class="s1">device=self.device)</span>

            <span class="s0">else</span><span class="s1">:</span>
                <span class="s4"># axis=1 because then we get size [batch_size, max_time, depth]</span>
                <span class="s1">encoded_edges = torch.stack(encoded_edges</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">1</span><span class="s1">)</span>
                <span class="s1">combined_edges</span><span class="s0">, </span><span class="s1">_ = self.node_modules[self.node_type + </span><span class="s3">'/edge_influence_encoder'</span><span class="s1">](encoded_edges</span><span class="s0">,</span>
                                                                                                  <span class="s1">node_history_encoder)</span>
                <span class="s1">combined_edges = F.dropout(combined_edges</span><span class="s0">,</span>
                                           <span class="s1">p=</span><span class="s2">1. </span><span class="s1">- self.hyperparams[</span><span class="s3">'rnn_kwargs'</span><span class="s1">][</span><span class="s3">'dropout_keep_prob'</span><span class="s1">]</span><span class="s0">,</span>
                                           <span class="s1">training=(mode == ModeKeys.TRAIN))</span>

        <span class="s0">return </span><span class="s1">combined_edges</span>

    <span class="s0">def </span><span class="s1">encode_node_future(self</span><span class="s0">, </span><span class="s1">mode</span><span class="s0">, </span><span class="s1">node_present</span><span class="s0">, </span><span class="s1">node_future) -&gt; torch.Tensor:</span>
        <span class="s5">&quot;&quot;&quot; 
        Encodes the node future (during training) using a bi-directional LSTM 
 
        :param mode: Mode in which the model is operated. E.g. Train, Eval, Predict. 
        :param node_present: Current state of the node. [bs, state] 
        :param node_future: Future states of the node. [bs, ph, state] 
        :return: Encoded future. 
        &quot;&quot;&quot;</span>
        <span class="s1">initial_h_model = self.node_modules[self.node_type + </span><span class="s3">'/node_future_encoder/initial_h'</span><span class="s1">]</span>
        <span class="s1">initial_c_model = self.node_modules[self.node_type + </span><span class="s3">'/node_future_encoder/initial_c'</span><span class="s1">]</span>

        <span class="s4"># Here we're initializing the forward hidden states,</span>
        <span class="s4"># but zeroing the backward ones.</span>
        <span class="s1">initial_h = initial_h_model(node_present)</span>
        <span class="s1">initial_h = torch.stack([initial_h</span><span class="s0">, </span><span class="s1">torch.zeros_like(initial_h</span><span class="s0">, </span><span class="s1">device=self.device)]</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>

        <span class="s1">initial_c = initial_c_model(node_present)</span>
        <span class="s1">initial_c = torch.stack([initial_c</span><span class="s0">, </span><span class="s1">torch.zeros_like(initial_c</span><span class="s0">, </span><span class="s1">device=self.device)]</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>

        <span class="s1">initial_state = (initial_h</span><span class="s0">, </span><span class="s1">initial_c)</span>

        <span class="s1">_</span><span class="s0">, </span><span class="s1">state = self.node_modules[self.node_type + </span><span class="s3">'/node_future_encoder'</span><span class="s1">](node_future</span><span class="s0">, </span><span class="s1">initial_state)</span>
        <span class="s1">state = unpack_RNN_state(state)</span>
        <span class="s1">state = F.dropout(state</span><span class="s0">,</span>
                          <span class="s1">p=</span><span class="s2">1. </span><span class="s1">- self.hyperparams[</span><span class="s3">'rnn_kwargs'</span><span class="s1">][</span><span class="s3">'dropout_keep_prob'</span><span class="s1">]</span><span class="s0">,</span>
                          <span class="s1">training=(mode == ModeKeys.TRAIN))</span>

        <span class="s0">return </span><span class="s1">state</span>

    <span class="s0">def </span><span class="s1">encode_robot_future(self</span><span class="s0">, </span><span class="s1">mode</span><span class="s0">, </span><span class="s1">robot_present</span><span class="s0">, </span><span class="s1">robot_future) -&gt; torch.Tensor:</span>
        <span class="s5">&quot;&quot;&quot; 
        Encodes the robot future (during training) using a bi-directional LSTM 
 
        :param mode: Mode in which the model is operated. E.g. Train, Eval, Predict. 
        :param robot_present: Current state of the robot. [bs, state] 
        :param robot_future: Future states of the robot. [bs, ph, state] 
        :return: Encoded future. 
        &quot;&quot;&quot;</span>
        <span class="s1">initial_h_model = self.node_modules[</span><span class="s3">'robot_future_encoder/initial_h'</span><span class="s1">]</span>
        <span class="s1">initial_c_model = self.node_modules[</span><span class="s3">'robot_future_encoder/initial_c'</span><span class="s1">]</span>

        <span class="s4"># Here we're initializing the forward hidden states,</span>
        <span class="s4"># but zeroing the backward ones.</span>
        <span class="s1">initial_h = initial_h_model(robot_present)</span>
        <span class="s1">initial_h = torch.stack([initial_h</span><span class="s0">, </span><span class="s1">torch.zeros_like(initial_h</span><span class="s0">, </span><span class="s1">device=self.device)]</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>

        <span class="s1">initial_c = initial_c_model(robot_present)</span>
        <span class="s1">initial_c = torch.stack([initial_c</span><span class="s0">, </span><span class="s1">torch.zeros_like(initial_c</span><span class="s0">, </span><span class="s1">device=self.device)]</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)</span>

        <span class="s1">initial_state = (initial_h</span><span class="s0">, </span><span class="s1">initial_c)</span>

        <span class="s1">_</span><span class="s0">, </span><span class="s1">state = self.node_modules[</span><span class="s3">'robot_future_encoder'</span><span class="s1">](robot_future</span><span class="s0">, </span><span class="s1">initial_state)</span>
        <span class="s1">state = unpack_RNN_state(state)</span>
        <span class="s1">state = F.dropout(state</span><span class="s0">,</span>
                          <span class="s1">p=</span><span class="s2">1. </span><span class="s1">- self.hyperparams[</span><span class="s3">'rnn_kwargs'</span><span class="s1">][</span><span class="s3">'dropout_keep_prob'</span><span class="s1">]</span><span class="s0">,</span>
                          <span class="s1">training=(mode == ModeKeys.TRAIN))</span>

        <span class="s0">return </span><span class="s1">state</span>

    <span class="s0">def </span><span class="s1">q_z_xy(self</span><span class="s0">, </span><span class="s1">mode</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">y_e) -&gt; torch.Tensor:</span>
        <span class="s5">r&quot;&quot;&quot; 
        .. math:: q_\phi(z \mid \mathbf{x}_i, \mathbf{y}_i) 
 
        :param mode: Mode in which the model is operated. E.g. Train, Eval, Predict. 
        :param x: Input / Condition tensor. 
        :param y_e: Encoded future tensor. 
        :return: Latent distribution of the CVAE. 
        &quot;&quot;&quot;</span>
        <span class="s1">xy = torch.cat([x</span><span class="s0">, </span><span class="s1">y_e]</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">1</span><span class="s1">)</span>

        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'q_z_xy_MLP_dims'</span><span class="s1">] </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">dense = self.node_modules[self.node_type + </span><span class="s3">'/q_z_xy'</span><span class="s1">]</span>
            <span class="s1">h = F.dropout(F.relu(dense(xy))</span><span class="s0">,</span>
                          <span class="s1">p=</span><span class="s2">1. </span><span class="s1">- self.hyperparams[</span><span class="s3">'MLP_dropout_keep_prob'</span><span class="s1">]</span><span class="s0">,</span>
                          <span class="s1">training=(mode == ModeKeys.TRAIN))</span>

        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">h = xy</span>

        <span class="s1">to_latent = self.node_modules[self.node_type + </span><span class="s3">'/hxy_to_z'</span><span class="s1">]</span>
        <span class="s0">return </span><span class="s1">self.latent.dist_from_h(to_latent(h)</span><span class="s0">, </span><span class="s1">mode)</span>

    <span class="s0">def </span><span class="s1">p_z_x(self</span><span class="s0">, </span><span class="s1">mode</span><span class="s0">, </span><span class="s1">x):</span>
        <span class="s5">r&quot;&quot;&quot; 
        .. math:: p_\theta(z \mid \mathbf{x}_i) 
 
        :param mode: Mode in which the model is operated. E.g. Train, Eval, Predict. 
        :param x: Input / Condition tensor. 
        :return: Latent distribution of the CVAE. 
        &quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'p_z_x_MLP_dims'</span><span class="s1">] </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">dense = self.node_modules[self.node_type + </span><span class="s3">'/p_z_x'</span><span class="s1">]</span>
            <span class="s1">h = F.dropout(F.relu(dense(x))</span><span class="s0">,</span>
                          <span class="s1">p=</span><span class="s2">1. </span><span class="s1">- self.hyperparams[</span><span class="s3">'MLP_dropout_keep_prob'</span><span class="s1">]</span><span class="s0">,</span>
                          <span class="s1">training=(mode == ModeKeys.TRAIN))</span>

        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">h = x</span>

        <span class="s1">to_latent = self.node_modules[self.node_type + </span><span class="s3">'/hx_to_z'</span><span class="s1">]</span>
        <span class="s0">return </span><span class="s1">self.latent.dist_from_h(to_latent(h)</span><span class="s0">, </span><span class="s1">mode)</span>

    <span class="s0">def </span><span class="s1">project_to_GMM_params(self</span><span class="s0">, </span><span class="s1">tensor) -&gt; (torch.Tensor</span><span class="s0">, </span><span class="s1">torch.Tensor</span><span class="s0">, </span><span class="s1">torch.Tensor</span><span class="s0">, </span><span class="s1">torch.Tensor):</span>
        <span class="s5">&quot;&quot;&quot; 
        Projects tensor to parameters of a GMM with N components and D dimensions. 
 
        :param tensor: Input tensor. 
        :return: tuple(log_pis, mus, log_sigmas, corrs) 
            WHERE 
            - log_pis: Weight (logarithm) of each GMM component. [N] 
            - mus: Mean of each GMM component. [N, D] 
            - log_sigmas: Standard Deviation (logarithm) of each GMM component. [N, D] 
            - corrs: Correlation between the GMM components. [N] 
        &quot;&quot;&quot;</span>
        <span class="s1">log_pis = self.node_modules[self.node_type + </span><span class="s3">'/decoder/proj_to_GMM_log_pis'</span><span class="s1">](tensor)</span>
        <span class="s1">mus = self.node_modules[self.node_type + </span><span class="s3">'/decoder/proj_to_GMM_mus'</span><span class="s1">](tensor)</span>
        <span class="s1">log_sigmas = self.node_modules[self.node_type + </span><span class="s3">'/decoder/proj_to_GMM_log_sigmas'</span><span class="s1">](tensor)</span>
        <span class="s1">corrs = torch.tanh(self.node_modules[self.node_type + </span><span class="s3">'/decoder/proj_to_GMM_corrs'</span><span class="s1">](tensor))</span>
        <span class="s0">return </span><span class="s1">log_pis</span><span class="s0">, </span><span class="s1">mus</span><span class="s0">, </span><span class="s1">log_sigmas</span><span class="s0">, </span><span class="s1">corrs</span>

    <span class="s0">def </span><span class="s1">p_y_xz(self</span><span class="s0">, </span><span class="s1">mode</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">x_nr_t</span><span class="s0">, </span><span class="s1">y_r</span><span class="s0">, </span><span class="s1">n_s_t0</span><span class="s0">, </span><span class="s1">z_stacked</span><span class="s0">, </span><span class="s1">prediction_horizon</span><span class="s0">,</span>
               <span class="s1">num_samples</span><span class="s0">, </span><span class="s1">num_components=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">gmm_mode=</span><span class="s0">False</span><span class="s1">):</span>
        <span class="s5">r&quot;&quot;&quot; 
        .. math:: p_\psi(\mathbf{y}_i \mid \mathbf{x}_i, z) 
 
        :param mode: Mode in which the model is operated. E.g. Train, Eval, Predict. 
        :param x: Input / Condition tensor. 
        :param x_nr_t: Joint state of node and robot (if robot is in scene). 
        :param y: Future tensor. 
        :param y_r: Encoded future tensor. 
        :param n_s_t0: Standardized current state of the node. 
        :param z_stacked: Stacked latent state. [num_samples_z * num_samples_gmm, bs, latent_state] 
        :param prediction_horizon: Number of prediction timesteps. 
        :param num_samples: Number of samples from the latent space. 
        :param num_components: Number of GMM components. 
        :param gmm_mode: If True: The mode of the GMM is sampled. 
        :return: GMM2D. If mode is Predict, also samples from the GMM. 
        &quot;&quot;&quot;</span>
        <span class="s1">ph = prediction_horizon</span>
        <span class="s1">pred_dim = self.pred_state_length</span>

        <span class="s1">z = torch.reshape(z_stacked</span><span class="s0">, </span><span class="s1">(-</span><span class="s2">1</span><span class="s0">, </span><span class="s1">self.latent.z_dim))</span>
        <span class="s1">zx = torch.cat([z</span><span class="s0">, </span><span class="s1">x.repeat(num_samples * num_components</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)]</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">1</span><span class="s1">)</span>

        <span class="s1">cell = self.node_modules[self.node_type + </span><span class="s3">'/decoder/rnn_cell'</span><span class="s1">]</span>
        <span class="s1">initial_h_model = self.node_modules[self.node_type + </span><span class="s3">'/decoder/initial_h'</span><span class="s1">]</span>

        <span class="s1">initial_state = initial_h_model(zx)</span>

        <span class="s1">log_pis</span><span class="s0">, </span><span class="s1">mus</span><span class="s0">, </span><span class="s1">log_sigmas</span><span class="s0">, </span><span class="s1">corrs</span><span class="s0">, </span><span class="s1">a_sample = []</span><span class="s0">, </span><span class="s1">[]</span><span class="s0">, </span><span class="s1">[]</span><span class="s0">, </span><span class="s1">[]</span><span class="s0">, </span><span class="s1">[]</span>

        <span class="s4"># Infer initial action state for node from current state</span>
        <span class="s1">a_0 = self.node_modules[self.node_type + </span><span class="s3">'/decoder/state_action'</span><span class="s1">](n_s_t0)</span>

        <span class="s1">state = initial_state</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'incl_robot_node'</span><span class="s1">]:</span>
            <span class="s1">input_ = torch.cat([zx</span><span class="s0">,</span>
                                <span class="s1">a_0.repeat(num_samples * num_components</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)</span><span class="s0">,</span>
                                <span class="s1">x_nr_t.repeat(num_samples * num_components</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)]</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">1</span><span class="s1">)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">input_ = torch.cat([zx</span><span class="s0">, </span><span class="s1">a_0.repeat(num_samples * num_components</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)]</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">1</span><span class="s1">)</span>

        <span class="s0">for </span><span class="s1">j </span><span class="s0">in </span><span class="s1">range(ph):</span>
            <span class="s1">h_state = cell(input_</span><span class="s0">, </span><span class="s1">state)</span>
            <span class="s1">log_pi_t</span><span class="s0">, </span><span class="s1">mu_t</span><span class="s0">, </span><span class="s1">log_sigma_t</span><span class="s0">, </span><span class="s1">corr_t = self.project_to_GMM_params(h_state)</span>

            <span class="s1">gmm = GMM2D(log_pi_t</span><span class="s0">, </span><span class="s1">mu_t</span><span class="s0">, </span><span class="s1">log_sigma_t</span><span class="s0">, </span><span class="s1">corr_t)  </span><span class="s4"># [k;bs, pred_dim]</span>

            <span class="s0">if </span><span class="s1">mode == ModeKeys.PREDICT </span><span class="s0">and </span><span class="s1">gmm_mode:</span>
                <span class="s1">a_t = gmm.mode()</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">a_t = gmm.rsample()</span>

            <span class="s0">if </span><span class="s1">num_components &gt; </span><span class="s2">1</span><span class="s1">:</span>
                <span class="s0">if </span><span class="s1">mode == ModeKeys.PREDICT:</span>
                    <span class="s1">log_pis.append(self.latent.p_dist.logits.repeat(num_samples</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">))</span>
                <span class="s0">else</span><span class="s1">:</span>
                    <span class="s1">log_pis.append(self.latent.q_dist.logits.repeat(num_samples</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">))</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">log_pis.append(</span>
                    <span class="s1">torch.ones_like(corr_t.reshape(num_samples</span><span class="s0">, </span><span class="s1">num_components</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s1">).permute(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">1</span><span class="s1">).reshape(-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">1</span><span class="s1">))</span>
                <span class="s1">)</span>

            <span class="s1">mus.append(</span>
                <span class="s1">mu_t.reshape(</span>
                    <span class="s1">num_samples</span><span class="s0">, </span><span class="s1">num_components</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span>
                <span class="s1">).permute(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">3</span><span class="s1">).reshape(-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2 </span><span class="s1">* num_components)</span>
            <span class="s1">)</span>
            <span class="s1">log_sigmas.append(</span>
                <span class="s1">log_sigma_t.reshape(</span>
                    <span class="s1">num_samples</span><span class="s0">, </span><span class="s1">num_components</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2</span>
                <span class="s1">).permute(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">1</span><span class="s0">, </span><span class="s2">3</span><span class="s1">).reshape(-</span><span class="s2">1</span><span class="s0">, </span><span class="s2">2 </span><span class="s1">* num_components))</span>
            <span class="s1">corrs.append(</span>
                <span class="s1">corr_t.reshape(</span>
                    <span class="s1">num_samples</span><span class="s0">, </span><span class="s1">num_components</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span>
                <span class="s1">).permute(</span><span class="s2">0</span><span class="s0">, </span><span class="s2">2</span><span class="s0">, </span><span class="s2">1</span><span class="s1">).reshape(-</span><span class="s2">1</span><span class="s0">, </span><span class="s1">num_components))</span>

            <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'incl_robot_node'</span><span class="s1">]:</span>
                <span class="s1">dec_inputs = [zx</span><span class="s0">, </span><span class="s1">a_t</span><span class="s0">, </span><span class="s1">y_r[:</span><span class="s0">, </span><span class="s1">j].repeat(num_samples * num_components</span><span class="s0">, </span><span class="s2">1</span><span class="s1">)]</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">dec_inputs = [zx</span><span class="s0">, </span><span class="s1">a_t]</span>
            <span class="s1">input_ = torch.cat(dec_inputs</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">1</span><span class="s1">)</span>
            <span class="s1">state = h_state</span>

        <span class="s1">log_pis = torch.stack(log_pis</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">1</span><span class="s1">)</span>
        <span class="s1">mus = torch.stack(mus</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">1</span><span class="s1">)</span>
        <span class="s1">log_sigmas = torch.stack(log_sigmas</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">1</span><span class="s1">)</span>
        <span class="s1">corrs = torch.stack(corrs</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">1</span><span class="s1">)</span>

        <span class="s1">a_dist = GMM2D(torch.reshape(log_pis</span><span class="s0">, </span><span class="s1">[num_samples</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s1">ph</span><span class="s0">, </span><span class="s1">num_components])</span><span class="s0">,</span>
                       <span class="s1">torch.reshape(mus</span><span class="s0">, </span><span class="s1">[num_samples</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s1">ph</span><span class="s0">, </span><span class="s1">num_components * pred_dim])</span><span class="s0">,</span>
                       <span class="s1">torch.reshape(log_sigmas</span><span class="s0">, </span><span class="s1">[num_samples</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s1">ph</span><span class="s0">, </span><span class="s1">num_components * pred_dim])</span><span class="s0">,</span>
                       <span class="s1">torch.reshape(corrs</span><span class="s0">, </span><span class="s1">[num_samples</span><span class="s0">, </span><span class="s1">-</span><span class="s2">1</span><span class="s0">, </span><span class="s1">ph</span><span class="s0">, </span><span class="s1">num_components]))</span>

        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'dynamic'</span><span class="s1">][self.node_type][</span><span class="s3">'distribution'</span><span class="s1">]:</span>
            <span class="s1">y_dist = self.dynamic.integrate_distribution(a_dist</span><span class="s0">, </span><span class="s1">x)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">y_dist = a_dist</span>

        <span class="s0">if </span><span class="s1">mode == ModeKeys.PREDICT:</span>
            <span class="s0">if </span><span class="s1">gmm_mode:</span>
                <span class="s1">a_sample = a_dist.mode()</span>
            <span class="s0">else</span><span class="s1">:</span>
                <span class="s1">a_sample = a_dist.rsample()</span>
            <span class="s1">sampled_future = self.dynamic.integrate_samples(a_sample</span><span class="s0">, </span><span class="s1">x)</span>
            <span class="s0">return </span><span class="s1">y_dist</span><span class="s0">, </span><span class="s1">sampled_future</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s0">return </span><span class="s1">y_dist</span>

    <span class="s0">def </span><span class="s1">encoder(self</span><span class="s0">, </span><span class="s1">mode</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">y_e</span><span class="s0">, </span><span class="s1">num_samples=</span><span class="s0">None</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Encoder of the CVAE. 
 
        :param mode: Mode in which the model is operated. E.g. Train, Eval, Predict. 
        :param x: Input / Condition tensor. 
        :param y_e: Encoded future tensor. 
        :param num_samples: Number of samples from the latent space during Prediction. 
        :return: tuple(z, kl_obj) 
            WHERE 
            - z: Samples from the latent space. 
            - kl_obj: KL Divergenze between q and p 
        &quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">mode == ModeKeys.TRAIN:</span>
            <span class="s1">sample_ct = self.hyperparams[</span><span class="s3">'k'</span><span class="s1">]</span>
        <span class="s0">elif </span><span class="s1">mode == ModeKeys.EVAL:</span>
            <span class="s1">sample_ct = self.hyperparams[</span><span class="s3">'k_eval'</span><span class="s1">]</span>
        <span class="s0">elif </span><span class="s1">mode == ModeKeys.PREDICT:</span>
            <span class="s1">sample_ct = num_samples</span>
            <span class="s0">if </span><span class="s1">num_samples </span><span class="s0">is None</span><span class="s1">:</span>
                <span class="s0">raise </span><span class="s1">ValueError(</span><span class="s3">&quot;num_samples cannot be None with mode == PREDICT.&quot;</span><span class="s1">)</span>

        <span class="s1">self.latent.q_dist = self.q_z_xy(mode</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">y_e)</span>
        <span class="s1">self.latent.p_dist = self.p_z_x(mode</span><span class="s0">, </span><span class="s1">x)</span>

        <span class="s1">z = self.latent.sample_q(sample_ct</span><span class="s0">, </span><span class="s1">mode)</span>

        <span class="s0">if </span><span class="s1">mode == ModeKeys.TRAIN:</span>
            <span class="s1">kl_obj = self.latent.kl_q_p(self.log_writer</span><span class="s0">, </span><span class="s3">'%s' </span><span class="s1">% str(self.node_type)</span><span class="s0">, </span><span class="s1">self.curr_iter)</span>
            <span class="s0">if </span><span class="s1">self.log_writer </span><span class="s0">is not None</span><span class="s1">:</span>
                <span class="s1">self.log_writer.add_scalar(</span><span class="s3">'%s/%s' </span><span class="s1">% (str(self.node_type)</span><span class="s0">, </span><span class="s3">'kl'</span><span class="s1">)</span><span class="s0">, </span><span class="s1">kl_obj</span><span class="s0">, </span><span class="s1">self.curr_iter)</span>
        <span class="s0">else</span><span class="s1">:</span>
            <span class="s1">kl_obj = </span><span class="s0">None</span>

        <span class="s0">return </span><span class="s1">z</span><span class="s0">, </span><span class="s1">kl_obj</span>

    <span class="s0">def </span><span class="s1">decoder(self</span><span class="s0">, </span><span class="s1">mode</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">x_nr_t</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">y_r</span><span class="s0">, </span><span class="s1">n_s_t0</span><span class="s0">, </span><span class="s1">z</span><span class="s0">, </span><span class="s1">labels</span><span class="s0">, </span><span class="s1">prediction_horizon</span><span class="s0">, </span><span class="s1">num_samples):</span>
        <span class="s5">&quot;&quot;&quot; 
        Decoder of the CVAE. 
 
        :param mode: Mode in which the model is operated. E.g. Train, Eval, Predict. 
        :param x: Input / Condition tensor. 
        :param x: Input / Condition tensor. 
        :param x_nr_t: Joint state of node and robot (if robot is in scene). 
        :param y: Future tensor. 
        :param y_r: Encoded future tensor. 
        :param n_s_t0: Standardized current state of the node. 
        :param z: Stacked latent state. 
        :param prediction_horizon: Number of prediction timesteps. 
        :param num_samples: Number of samples from the latent space. 
        :return: Log probability of y over p. 
        &quot;&quot;&quot;</span>

        <span class="s1">num_components = self.hyperparams[</span><span class="s3">'N'</span><span class="s1">] * self.hyperparams[</span><span class="s3">'K'</span><span class="s1">]</span>
        <span class="s1">y_dist = self.p_y_xz(mode</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">x_nr_t</span><span class="s0">, </span><span class="s1">y_r</span><span class="s0">, </span><span class="s1">n_s_t0</span><span class="s0">, </span><span class="s1">z</span><span class="s0">,</span>
                             <span class="s1">prediction_horizon</span><span class="s0">, </span><span class="s1">num_samples</span><span class="s0">, </span><span class="s1">num_components=num_components)</span>
        <span class="s1">log_p_yt_xz = torch.clamp(y_dist.log_prob(labels)</span><span class="s0">, </span><span class="s1">max=self.hyperparams[</span><span class="s3">'log_p_yt_xz_max'</span><span class="s1">])</span>
        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'log_histograms'</span><span class="s1">] </span><span class="s0">and </span><span class="s1">self.log_writer </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self.log_writer.add_histogram(</span><span class="s3">'%s/%s' </span><span class="s1">% (str(self.node_type)</span><span class="s0">, </span><span class="s3">'log_p_yt_xz'</span><span class="s1">)</span><span class="s0">, </span><span class="s1">log_p_yt_xz</span><span class="s0">, </span><span class="s1">self.curr_iter)</span>

        <span class="s1">log_p_y_xz = torch.sum(log_p_yt_xz</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">2</span><span class="s1">)</span>
        <span class="s0">return </span><span class="s1">log_p_y_xz</span>

    <span class="s0">def </span><span class="s1">train_loss(self</span><span class="s0">,</span>
                   <span class="s1">inputs</span><span class="s0">,</span>
                   <span class="s1">inputs_st</span><span class="s0">,</span>
                   <span class="s1">first_history_indices</span><span class="s0">,</span>
                   <span class="s1">labels</span><span class="s0">,</span>
                   <span class="s1">labels_st</span><span class="s0">,</span>
                   <span class="s1">neighbors</span><span class="s0">,</span>
                   <span class="s1">neighbors_edge_value</span><span class="s0">,</span>
                   <span class="s1">robot</span><span class="s0">,</span>
                   <span class="s1">map</span><span class="s0">,</span>
                   <span class="s1">prediction_horizon) -&gt; torch.Tensor:</span>
        <span class="s5">&quot;&quot;&quot; 
        Calculates the training loss for a batch. 
 
        :param inputs: Input tensor including the state for each agent over time [bs, t, state]. 
        :param inputs_st: Standardized input tensor. 
        :param first_history_indices: First timestep (index) in scene for which data is available for a node [bs] 
        :param labels: Label tensor including the label output for each agent over time [bs, t, pred_state]. 
        :param labels_st: Standardized label tensor. 
        :param neighbors: Preprocessed dict (indexed by edge type) of list of neighbor states over time. 
                            [[bs, t, neighbor state]] 
        :param neighbors_edge_value: Preprocessed edge values for all neighbor nodes [[N]] 
        :param robot: Standardized robot state over time. [bs, t, robot_state] 
        :param map: Tensor of Map information. [bs, channels, x, y] 
        :param prediction_horizon: Number of prediction timesteps. 
        :return: Scalar tensor -&gt; nll loss 
        &quot;&quot;&quot;</span>
        <span class="s1">mode = ModeKeys.TRAIN</span>

        <span class="s1">x</span><span class="s0">, </span><span class="s1">x_nr_t</span><span class="s0">, </span><span class="s1">y_e</span><span class="s0">, </span><span class="s1">y_r</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">n_s_t0 = self.obtain_encoded_tensors(mode=mode</span><span class="s0">,</span>
                                                                     <span class="s1">inputs=inputs</span><span class="s0">,</span>
                                                                     <span class="s1">inputs_st=inputs_st</span><span class="s0">,</span>
                                                                     <span class="s1">labels=labels</span><span class="s0">,</span>
                                                                     <span class="s1">labels_st=labels_st</span><span class="s0">,</span>
                                                                     <span class="s1">first_history_indices=first_history_indices</span><span class="s0">,</span>
                                                                     <span class="s1">neighbors=neighbors</span><span class="s0">,</span>
                                                                     <span class="s1">neighbors_edge_value=neighbors_edge_value</span><span class="s0">,</span>
                                                                     <span class="s1">robot=robot</span><span class="s0">,</span>
                                                                     <span class="s1">map=map)</span>

        <span class="s1">z</span><span class="s0">, </span><span class="s1">kl = self.encoder(mode</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">y_e)</span>
        <span class="s1">log_p_y_xz = self.decoder(mode</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">x_nr_t</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">y_r</span><span class="s0">, </span><span class="s1">n_s_t0</span><span class="s0">, </span><span class="s1">z</span><span class="s0">,</span>
                                  <span class="s1">labels</span><span class="s0">,  </span><span class="s4"># Loss is calculated on unstandardized label</span>
                                  <span class="s1">prediction_horizon</span><span class="s0">,</span>
                                  <span class="s1">self.hyperparams[</span><span class="s3">'k'</span><span class="s1">])</span>

        <span class="s1">log_p_y_xz_mean = torch.mean(log_p_y_xz</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)  </span><span class="s4"># [nbs]</span>
        <span class="s1">log_likelihood = torch.mean(log_p_y_xz_mean)</span>

        <span class="s1">mutual_inf_q = mutual_inf_mc(self.latent.q_dist)</span>
        <span class="s1">mutual_inf_p = mutual_inf_mc(self.latent.p_dist)</span>

        <span class="s1">ELBO = log_likelihood - self.kl_weight * kl + </span><span class="s2">1. </span><span class="s1">* mutual_inf_p</span>
        <span class="s1">loss = -ELBO</span>

        <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'log_histograms'</span><span class="s1">] </span><span class="s0">and </span><span class="s1">self.log_writer </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self.log_writer.add_histogram(</span><span class="s3">'%s/%s' </span><span class="s1">% (str(self.node_type)</span><span class="s0">, </span><span class="s3">'log_p_y_xz'</span><span class="s1">)</span><span class="s0">,</span>
                                          <span class="s1">log_p_y_xz_mean</span><span class="s0">,</span>
                                          <span class="s1">self.curr_iter)</span>

        <span class="s0">if </span><span class="s1">self.log_writer </span><span class="s0">is not None</span><span class="s1">:</span>
            <span class="s1">self.log_writer.add_scalar(</span><span class="s3">'%s/%s' </span><span class="s1">% (str(self.node_type)</span><span class="s0">, </span><span class="s3">'mutual_information_q'</span><span class="s1">)</span><span class="s0">,</span>
                                       <span class="s1">mutual_inf_q</span><span class="s0">,</span>
                                       <span class="s1">self.curr_iter)</span>
            <span class="s1">self.log_writer.add_scalar(</span><span class="s3">'%s/%s' </span><span class="s1">% (str(self.node_type)</span><span class="s0">, </span><span class="s3">'mutual_information_p'</span><span class="s1">)</span><span class="s0">,</span>
                                       <span class="s1">mutual_inf_p</span><span class="s0">,</span>
                                       <span class="s1">self.curr_iter)</span>
            <span class="s1">self.log_writer.add_scalar(</span><span class="s3">'%s/%s' </span><span class="s1">% (str(self.node_type)</span><span class="s0">, </span><span class="s3">'log_likelihood'</span><span class="s1">)</span><span class="s0">,</span>
                                       <span class="s1">log_likelihood</span><span class="s0">,</span>
                                       <span class="s1">self.curr_iter)</span>
            <span class="s1">self.log_writer.add_scalar(</span><span class="s3">'%s/%s' </span><span class="s1">% (str(self.node_type)</span><span class="s0">, </span><span class="s3">'loss'</span><span class="s1">)</span><span class="s0">,</span>
                                       <span class="s1">loss</span><span class="s0">,</span>
                                       <span class="s1">self.curr_iter)</span>
            <span class="s0">if </span><span class="s1">self.hyperparams[</span><span class="s3">'log_histograms'</span><span class="s1">]:</span>
                <span class="s1">self.latent.summarize_for_tensorboard(self.log_writer</span><span class="s0">, </span><span class="s1">str(self.node_type)</span><span class="s0">, </span><span class="s1">self.curr_iter)</span>
        <span class="s0">return </span><span class="s1">loss</span>

    <span class="s0">def </span><span class="s1">eval_loss(self</span><span class="s0">,</span>
                  <span class="s1">inputs</span><span class="s0">,</span>
                  <span class="s1">inputs_st</span><span class="s0">,</span>
                  <span class="s1">first_history_indices</span><span class="s0">,</span>
                  <span class="s1">labels</span><span class="s0">,</span>
                  <span class="s1">labels_st</span><span class="s0">,</span>
                  <span class="s1">neighbors</span><span class="s0">,</span>
                  <span class="s1">neighbors_edge_value</span><span class="s0">,</span>
                  <span class="s1">robot</span><span class="s0">,</span>
                  <span class="s1">map</span><span class="s0">,</span>
                  <span class="s1">prediction_horizon) -&gt; torch.Tensor:</span>
        <span class="s5">&quot;&quot;&quot; 
        Calculates the evaluation loss for a batch. 
 
        :param inputs: Input tensor including the state for each agent over time [bs, t, state]. 
        :param inputs_st: Standardized input tensor. 
        :param first_history_indices: First timestep (index) in scene for which data is available for a node [bs] 
        :param labels: Label tensor including the label output for each agent over time [bs, t, pred_state]. 
        :param labels_st: Standardized label tensor. 
        :param neighbors: Preprocessed dict (indexed by edge type) of list of neighbor states over time. 
                            [[bs, t, neighbor state]] 
        :param neighbors_edge_value: Preprocessed edge values for all neighbor nodes [[N]] 
        :param robot: Standardized robot state over time. [bs, t, robot_state] 
        :param map: Tensor of Map information. [bs, channels, x, y] 
        :param prediction_horizon: Number of prediction timesteps. 
        :return: tuple(nll_q_is, nll_p, nll_exact, nll_sampled) 
        &quot;&quot;&quot;</span>

        <span class="s1">mode = ModeKeys.EVAL</span>

        <span class="s1">x</span><span class="s0">, </span><span class="s1">x_nr_t</span><span class="s0">, </span><span class="s1">y_e</span><span class="s0">, </span><span class="s1">y_r</span><span class="s0">, </span><span class="s1">y</span><span class="s0">, </span><span class="s1">n_s_t0 = self.obtain_encoded_tensors(mode=mode</span><span class="s0">,</span>
                                                                     <span class="s1">inputs=inputs</span><span class="s0">,</span>
                                                                     <span class="s1">inputs_st=inputs_st</span><span class="s0">,</span>
                                                                     <span class="s1">labels=labels</span><span class="s0">,</span>
                                                                     <span class="s1">labels_st=labels_st</span><span class="s0">,</span>
                                                                     <span class="s1">first_history_indices=first_history_indices</span><span class="s0">,</span>
                                                                     <span class="s1">neighbors=neighbors</span><span class="s0">,</span>
                                                                     <span class="s1">neighbors_edge_value=neighbors_edge_value</span><span class="s0">,</span>
                                                                     <span class="s1">robot=robot</span><span class="s0">,</span>
                                                                     <span class="s1">map=map)</span>

        <span class="s1">num_components = self.hyperparams[</span><span class="s3">'N'</span><span class="s1">] * self.hyperparams[</span><span class="s3">'K'</span><span class="s1">]</span>
        <span class="s4">### Importance sampled NLL estimate</span>
        <span class="s1">z</span><span class="s0">, </span><span class="s1">_ = self.encoder(mode</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">y_e)  </span><span class="s4"># [k_eval, nbs, N*K]</span>
        <span class="s1">z = self.latent.sample_p(</span><span class="s2">1</span><span class="s0">, </span><span class="s1">mode</span><span class="s0">, </span><span class="s1">full_dist=</span><span class="s0">True</span><span class="s1">)</span>
        <span class="s1">y_dist</span><span class="s0">, </span><span class="s1">_ = self.p_y_xz(ModeKeys.PREDICT</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">x_nr_t</span><span class="s0">, </span><span class="s1">y_r</span><span class="s0">, </span><span class="s1">n_s_t0</span><span class="s0">, </span><span class="s1">z</span><span class="s0">,</span>
                                <span class="s1">prediction_horizon</span><span class="s0">, </span><span class="s1">num_samples=</span><span class="s2">1</span><span class="s0">, </span><span class="s1">num_components=num_components)</span>
        <span class="s4"># We use unstandardized labels to compute the loss</span>
        <span class="s1">log_p_yt_xz = torch.clamp(y_dist.log_prob(labels)</span><span class="s0">, </span><span class="s1">max=self.hyperparams[</span><span class="s3">'log_p_yt_xz_max'</span><span class="s1">])</span>
        <span class="s1">log_p_y_xz = torch.sum(log_p_yt_xz</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">2</span><span class="s1">)</span>
        <span class="s1">log_p_y_xz_mean = torch.mean(log_p_y_xz</span><span class="s0">, </span><span class="s1">dim=</span><span class="s2">0</span><span class="s1">)  </span><span class="s4"># [nbs]</span>
        <span class="s1">log_likelihood = torch.mean(log_p_y_xz_mean)</span>
        <span class="s1">nll = -log_likelihood</span>

        <span class="s0">return </span><span class="s1">nll</span>

    <span class="s0">def </span><span class="s1">predict(self</span><span class="s0">,</span>
                <span class="s1">inputs</span><span class="s0">,</span>
                <span class="s1">inputs_st</span><span class="s0">,</span>
                <span class="s1">first_history_indices</span><span class="s0">,</span>
                <span class="s1">neighbors</span><span class="s0">,</span>
                <span class="s1">neighbors_edge_value</span><span class="s0">,</span>
                <span class="s1">robot</span><span class="s0">,</span>
                <span class="s1">map</span><span class="s0">,</span>
                <span class="s1">prediction_horizon</span><span class="s0">,</span>
                <span class="s1">num_samples</span><span class="s0">,</span>
                <span class="s1">z_mode=</span><span class="s0">False,</span>
                <span class="s1">gmm_mode=</span><span class="s0">False,</span>
                <span class="s1">full_dist=</span><span class="s0">True,</span>
                <span class="s1">all_z_sep=</span><span class="s0">False</span><span class="s1">):</span>
        <span class="s5">&quot;&quot;&quot; 
        Predicts the future of a batch of nodes. 
 
        :param inputs: Input tensor including the state for each agent over time [bs, t, state]. 
        :param inputs_st: Standardized input tensor. 
        :param first_history_indices: First timestep (index) in scene for which data is available for a node [bs] 
        :param neighbors: Preprocessed dict (indexed by edge type) of list of neighbor states over time. 
                            [[bs, t, neighbor state]] 
        :param neighbors_edge_value: Preprocessed edge values for all neighbor nodes [[N]] 
        :param robot: Standardized robot state over time. [bs, t, robot_state] 
        :param map: Tensor of Map information. [bs, channels, x, y] 
        :param prediction_horizon: Number of prediction timesteps. 
        :param num_samples: Number of samples from the latent space. 
        :param z_mode: If True: Select the most likely latent state. 
        :param gmm_mode: If True: The mode of the GMM is sampled. 
        :param all_z_sep: Samples each latent mode individually without merging them into a GMM. 
        :param full_dist: Samples all latent states and merges them into a GMM as output. 
        :return: 
        &quot;&quot;&quot;</span>
        <span class="s1">mode = ModeKeys.PREDICT</span>

        <span class="s1">x</span><span class="s0">, </span><span class="s1">x_nr_t</span><span class="s0">, </span><span class="s1">_</span><span class="s0">, </span><span class="s1">y_r</span><span class="s0">, </span><span class="s1">_</span><span class="s0">, </span><span class="s1">n_s_t0 = self.obtain_encoded_tensors(mode=mode</span><span class="s0">,</span>
                                                                   <span class="s1">inputs=inputs</span><span class="s0">,</span>
                                                                   <span class="s1">inputs_st=inputs_st</span><span class="s0">,</span>
                                                                   <span class="s1">labels=</span><span class="s0">None,</span>
                                                                   <span class="s1">labels_st=</span><span class="s0">None,</span>
                                                                   <span class="s1">first_history_indices=first_history_indices</span><span class="s0">,</span>
                                                                   <span class="s1">neighbors=neighbors</span><span class="s0">,</span>
                                                                   <span class="s1">neighbors_edge_value=neighbors_edge_value</span><span class="s0">,</span>
                                                                   <span class="s1">robot=robot</span><span class="s0">,</span>
                                                                   <span class="s1">map=map)</span>

        <span class="s1">self.latent.p_dist = self.p_z_x(mode</span><span class="s0">, </span><span class="s1">x)</span>
        <span class="s1">z</span><span class="s0">, </span><span class="s1">num_samples</span><span class="s0">, </span><span class="s1">num_components = self.latent.sample_p(num_samples</span><span class="s0">,</span>
                                                              <span class="s1">mode</span><span class="s0">,</span>
                                                              <span class="s1">most_likely_z=z_mode</span><span class="s0">,</span>
                                                              <span class="s1">full_dist=full_dist</span><span class="s0">,</span>
                                                              <span class="s1">all_z_sep=all_z_sep)</span>

        <span class="s1">_</span><span class="s0">, </span><span class="s1">our_sampled_future = self.p_y_xz(mode</span><span class="s0">, </span><span class="s1">x</span><span class="s0">, </span><span class="s1">x_nr_t</span><span class="s0">, </span><span class="s1">y_r</span><span class="s0">, </span><span class="s1">n_s_t0</span><span class="s0">, </span><span class="s1">z</span><span class="s0">,</span>
                                            <span class="s1">prediction_horizon</span><span class="s0">,</span>
                                            <span class="s1">num_samples</span><span class="s0">,</span>
                                            <span class="s1">num_components</span><span class="s0">,</span>
                                            <span class="s1">gmm_mode)</span>

        <span class="s0">return </span><span class="s1">our_sampled_future</span>
</pre>
</body>
</html>